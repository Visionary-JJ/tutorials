{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div style=\"font-size:2em\"> OpenAI Gym Workshop: Build A Reinforcement Learning Agent</div></center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: right;font-size:1.2em\"> Juxihong Julaiti (Jushkhun)@ LISA </div>\n",
    "<div style=\"text-align: right;font-size:1.2em\"> Ph.D. Candidate in IE&OR </div>\n",
    "<div style=\"text-align: right;font-size:1.2em\"> 9/19/2019 </div>\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    " \n",
    "# What is reinforcement learning (RL)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:1.2em\"> RL is a machine learning method in which an <b>agent</b> (or multiple agents) learns to find a <b>strategy/policy</b> by interacting with the <b>environment</b> in order to maximize the <b>expected reward</b> (or minimize expected penalty) associated with the environment</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components:\n",
    "\n",
    "- agent\n",
    "\n",
    "- environment\n",
    "\n",
    "- strategy/policy\n",
    "\n",
    "- expected reward\n",
    "\n",
    "## Examples\n",
    "\n",
    "<span style=\"font-size:1.2em\">\n",
    "<img src=\"files/break.PNG\" height=\"2%\" width=\"2%\">\n",
    "    \n",
    "- An robot agent learning how to ride a bike/walk to maximize the total distance moved within a time window\n",
    "\n",
    "- An robot agent learning to clean the house to maximize the cleanliness of the house and the energy cost of the robot\n",
    "\n",
    "- An agent learning how much to invest to stocks in a given set to maximize the total return\n",
    "\n",
    "- An CNC machine agent learning how to pick jobs to process in a manufacturing facility to maximize the number of on-time jobs\n",
    "\n",
    "...\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "\n",
    "<img src=\"files/car.gif\" height=\"50%\" width=\"50%\">\n",
    "\n",
    "A key characteristics of RL: Reward might be delayed\n",
    "\n",
    "- The agent might need to sacrifice immidiate rewards in order to get a good long term reward\n",
    "\n",
    "- Actions may have a long term concequences\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components:\n",
    "\n",
    "- <b>agent</b>\n",
    "\n",
    "- environment\n",
    "\n",
    "- strategy/policy\n",
    "\n",
    "- expected reward\n",
    "\n",
    "\n",
    "## Agent\n",
    "<img src=\"files/agent.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "\n",
    "\n",
    "Components:\n",
    "\n",
    "- agent\n",
    "\n",
    "- <b>environment</b>\n",
    "\n",
    "- strategy/policy\n",
    "\n",
    "- expected reward\n",
    "\n",
    "## Agent in an environment\n",
    "<img src=\"files/mdp.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "An example:\n",
    "\n",
    "<img src=\"files/example.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components:\n",
    "\n",
    "- agent\n",
    "\n",
    "- environment\n",
    "\n",
    "- <b>strategy/policy</b>\n",
    "\n",
    "- expected reward\n",
    "\n",
    "## Policy\n",
    "\n",
    "<center><div style=\"font-size:2em\"> $\\pi(a|s) = $Prob( take action $a$ | in state $s$ )$\\in [0,1]$</div></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"font-size:1.3em\"> Given any state, it tells the agent the probability of taking an action </div>\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:2em\"> Current State $S_t$:</div>\n",
    "\n",
    "\n",
    "<img src=\"files/policy.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "<div style=\"font-size:2em\"> $A$=[stay, go up, go down]</div>\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"font-size:2em\"> A random policy:</div>\n",
    "<img src=\"files/break.PNG\" height=\"10%\" width=\"10%\">\n",
    "<center><div style=\"font-size:1.5em\">$\\pi_($stay$|S_t)=\\pi_($go up$|S_t)=\\pi_($go down$|S_t)=\\frac{1}{3}$</div></center>\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"30%\" width=\"30%\">\n",
    "\n",
    "<div style=\"font-size:2em\"> A better policy:</div>\n",
    "<img src=\"files/break.PNG\" height=\"10%\" width=\"10%\">\n",
    "<center><div style=\"font-size:1.5em\">$\\pi_($stay$|S_t)=0$</div></center>\n",
    "\n",
    "<center><div style=\"font-size:1.5em\">$\\pi_($go up$|S_t)=0$</div></center>\n",
    "\n",
    "<center><div style=\"font-size:1.5em\">$\\pi_($go down$|S_t)=1$</div></center>\n",
    "\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components:\n",
    "\n",
    "- agent\n",
    "\n",
    "- environment\n",
    "\n",
    "- strategy/policy\n",
    "\n",
    "- <b>expected reward</b>\n",
    "\n",
    "\n",
    "## Expected reward\n",
    "<img src=\"files/break.PNG\" height=\"10%\" width=\"10%\">\n",
    "\n",
    "After one round of playing a game with policy $\\pi$, we can look at the trajectory:\n",
    "\n",
    "<center><div style=\"font-size:1.5em\"> $[S_0, A_0, R_0, S_1, A_1, R_1, ...., S_{T-1},A_{T-1},R_{T-1},S_T]$ </div></center>\n",
    "\n",
    "$S_T$ is the terminal State\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"10%\" width=\"10%\">\n",
    "\n",
    "<div style=\"font-size:1.5em\">$G_\\pi$: Total discounted rewards obtained by acting based on policy $\\pi$</div>\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"10%\" width=\"10%\">\n",
    "\n",
    "<div style=\"font-size:1.5em\"> $G_\\pi = \\sum_{t=0}^T \\gamma^t R_t, \\gamma \\in (0,1]$</div>\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"10%\" width=\"10%\">\n",
    "\n",
    "objective function:\n",
    "<center><div style=\"font-size:1.5em\"> $\\max_{\\pi} E[G_\\pi]$</div></center>\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"10%\" width=\"10%\">\n",
    "\n",
    "<div style=\"font-size:1.5em\"> Reinforcement learning is the process of finding a policy $\\pi$ that maximize the expected total rewards $E[G_\\pi]$ by trial and error</div>\n",
    "\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different reinforcement learning methods \n",
    "\n",
    "<img src=\"files/RL_classes.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "\n",
    "<img src=\"files/RL_methods.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n",
    "Justesen, Niels, et al. \"Deep learning for video game playing.\" IEEE Transactions on Games (2019).\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences of different learning\n",
    "\n",
    "| Supervised Learning| Unsupervised Learning |Reinforcement Learning|\n",
    "| ----------- | ----------- |----------- |\n",
    "| Learns a model that maps X into Y| Learn the underlying structure of the data|Learn policy via trial & error|\n",
    "| Needs the right answer (Y) to start   |         |Needs feedback on the agent’s action (reward signal)|\n",
    "| Inputs are independent and identically distributed| Inputs are independent and identically distributed|Agent’s decision changes the next input|\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/gym.PNG\" height=\"15%\" width=\"15%\">\n",
    "<center>Open AI Gym</center> \n",
    "Gym is a <b>toolkit</b> that provides you with <b>various simulators</b> so that you can focus on <b>developing and comparing</b> reinforcement learning <b>algorithms</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td><img src=\"files/lunarlander.gif\" height=\"80%\" width=\"80%\"></td><td><img src=\"files/spider.gif\" height=\"80%\" width=\"80%\"> </td>\n",
    "</tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td> <img src=\"files/cartpole.gif\" height=\"100%\" width=\"100%\"> </td>\n",
    "<td> <img src=\"files/atari.gif\" height=\"100%\" width=\"100%\"> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many more games, check out their website: https://gym.openai.com/envs/#robotics\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why gym?\n",
    "\n",
    "1. <b>Focus on research side of RL</b>:\n",
    "Prebuild <U>accurate and fast simulators(MDPs/POMDPs)</U> so that you can focus on algorithm testing without building simulators (which sometime can be quite tedious and very time-consuming)\n",
    "\n",
    "<br/>\n",
    "\n",
    "2. <b>Benchmarking, compare your algorithms with others</b>:\n",
    "Many of the environemtns of Gym are widely used in research papers.\n",
    "\n",
    "    - Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" <b>Nature</b> 518.7540 (2015): 529.\n",
    "\n",
    "    - Henderson, Peter, et al. \"Deep reinforcement learning that matters.\" Thirty-Second <b>AAAI</b> Conference on Artificial Intelligence. 2018.\n",
    "\n",
    "    - Christiano, Paul F., et al. \"Deep reinforcement learning from human preferences.\" Advances in Neural Information Processing Systems. 2017.\n",
    "    - Justesen, Niels, et al. \"Deep learning for video game playing.\" IEEE Transactions on Games (2019).\n",
    "\n",
    "...\n",
    "\n",
    "## Use cases in industries\n",
    "\n",
    "Bugs in the algorithms V.S. simulators:\n",
    "- RL algorithms are difficult to debug as the model can get very complex\n",
    "    - A bug in the simulator can cause the model divergent\n",
    "    - Passing tests in environemtns ensure the model is working, it will help you to narrow down the places you need to check\n",
    "- Testing procedure\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "## pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CartPole-v1\n",
    "#MountainCar-v0\n",
    "#Acrobot-v1\n",
    "#Pendulum-v0\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "s_t = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes of env.\n",
    "### Continuous state space & action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.observation_space.shape[0]\n",
    "#env.action_space.shape[0]\n",
    "print(\"env state space size: {} |action space size: {}\".format(env.observation_space.shape[0],env.action_space.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"state upper bound:{}\".format(env.observation_space.high))\n",
    "print(\"state lower bound:{}\".format(env.observation_space.low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"action upper bound:{}\".format(env.action_space.high))\n",
    "print(\"action lower bound:{}\".format(env.action_space.low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample state space:{}\".format(env.observation_space.sample()))\n",
    "print(\"sample action:{}\".format(env.action_space.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete state space & action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.n\n",
    "env.action_space.n\n",
    "print(\"env state space size: {} |action space size: {}\".format(env.observation_space.n,env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample state:{}\".format(env.observation_space.sample()))\n",
    "print(\"sample action:{}\".format(env.action_space.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with the env: env.step($a_t$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t=1\n",
    "env.step(a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_tp1,Rt,done,_=env.step(a_t)\n",
    "print(s_tp1,Rt,done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# pip install matplotlib\n",
    "# pip install numpy \n",
    "\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have installed anaconda\n",
    "\n",
    "windows user: \n",
    "    \n",
    "    open anaconda prompt, and do the above commands\n",
    "\n",
    "linux/mac user:\n",
    "\n",
    "    open a terminal, and do the above commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: random policy (Taxi)\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><div style=\"font-size:1.5em\"> $\\pi(a_t=0|S_t=s)=\\pi(a_t=1|S_t=s)=...=\\pi(a_t=5|S_t=s)=\\frac{1}{6}, \\forall s\\in S$</div></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states,num_actions=env.observation_space.n,env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.ones([num_states, num_actions]) / num_actions \n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy[0,4] # the probability of taking action 0 in state 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=0                                                 # init. the total reward\n",
    "\n",
    "s_t=env.reset()                                     # init. the state\n",
    "\n",
    "for _ in range(500):                                 # play 50 timestamp (not 50 runs)\n",
    "    \n",
    "    a_t=np.random.choice(num_actions,p=policy[s_t]) # sample action from the random policy defined above\n",
    "    \n",
    "    s_t,r_t,done,info=env.step(a_t)                 # update the state and get reward signal\n",
    "    \n",
    "    G+=r_t                                          # update the total reward\n",
    "    \n",
    "    if not done:\n",
    "        env.render()\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        print(\"Game over! The total reward is {}\".format(G))\n",
    "        break\n",
    "    time.sleep(0.1)                                 # pause for a moment so we will be able to see the game\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(policy,iterations):\n",
    "    \n",
    "    S_LOG,A_LOG,G_LOG=[],[],[]                             # will records the trajectories of all rounds\n",
    "    \n",
    "    for iter in range(iterations):                         # play 50 rounds\n",
    "        \n",
    "        G=0                                                # init. the total reward\n",
    "        \n",
    "        st_log,at_log=[],[]                                # will be used to record the trajectory\n",
    "        \n",
    "        s_t=env.reset()                                    # init. the state\n",
    "        \n",
    "        while True:                                        # keep playing until the game is over (time limit OR customer is dropped off)\n",
    "            \n",
    "            a_t=np.random.choice(num_actions,p=policy[s_t])# sample action from the random policy defined above\n",
    "            \n",
    "            s_tp1,r_t,done,info=env.step(a_t)              # update the state and get reward signal\n",
    "\n",
    "            st_log.append(s_t)                             # record the trajectory\n",
    "            at_log.append(a_t)\n",
    "             \n",
    "            G+=r_t                                         # update the total reward under policy pi\n",
    "\n",
    "            s_t=s_tp1                                      # update the state\n",
    "\n",
    "            if done:                                       # if game over update records\n",
    "                S_LOG.append(st_log)\n",
    "                A_LOG.append(at_log)\n",
    "                G_LOG.append(G)\n",
    "                break\n",
    "    return S_LOG,A_LOG,G_LOG                              # return the records of all rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_log,a_log,G_log =run(policy,iterations=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expect_G,Std_G=np.mean(G_log),np.std(G_log)\n",
    "plt.hist(G_log,bins=30)\n",
    "plt.vlines(np.mean(G_log),[0],[100],color=\"black\",label=\"mean reward\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Expect reward:{:5.2f} | Std reward:{:5.2f} | Max reward:{} | Min reward:{}\".\\\n",
    "      format(Expect_G,Std_G,max(G_log),min(G_log)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Methods (CEM)\n",
    "\n",
    "model-free method; policy-based method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_log[0][:5],a_log[0][:5],G_log[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "episode 0: $s_0, a_0, r_0, s_1, a_1, ...$\n",
    "\n",
    "episode 1: $s_0, a_0, r_0, s_1, a_1, ...$\n",
    "\n",
    "episode 2: $s_0, a_0, r_0, s_1, a_1, ...$\n",
    "\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "episode 0: $s_0, a_0, r_0, s_1, a_1, ...$ => $G_0=r_0+r_1+...=-713$\n",
    "\n",
    "episode 1: $s_0, a_0, r_0, s_1, a_1, ...$ => $G_1=r_0+r_1+....=-713$\n",
    "\n",
    "episode 2: $s_0, a_0, r_0, s_1, a_1, ...$ => $G_2=r_0+r_1+....=-785$\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expect_G,Std_G,threshold=np.mean(G_log),np.std(G_log),np.percentile(G_log,98)\n",
    "plt.hist(G_log,bins=30)\n",
    "plt.vlines(threshold,[0],[200],color=\"green\",label=\"98% percentile\")\n",
    "plt.vlines(Expect_G,[0],[200],color=\"black\",label=\"mean\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Expect reward:{:5.2f} | Std reward:{:5.2f} | Max reward:{} | Min reward:{}\".\\\n",
    "      format(Expect_G,Std_G,max(G_log),min(G_log)))\n",
    "print(\"98% percentile:{:5.2f}\".format(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to extract the elite episodes\n",
    "def select_elites(s_log,a_log,G_log,percentile=98):\n",
    "    \n",
    "    # calculate the threshold of the reward based on the percentile\n",
    "    reward_threshold = np.percentile(G_log,percentile)\n",
    "    \n",
    "    # extract the trajectories that surpass the reward threshold\n",
    "    elite_states  = [l2 for l1 in list(np.array(s_log)[np.array(G_log)>=reward_threshold]) for l2 in l1]\n",
    "    elite_actions = [l2 for l1 in list(np.array(a_log)[np.array(G_log)>=reward_threshold]) for l2 in l1]\n",
    "    elite_total_rewards=[l1 for l1 in list(np.array(G_log)[np.array(G_log)>=reward_threshold])]\n",
    "    \n",
    "    # return the elite trajectories\n",
    "    return elite_states,elite_actions,elite_total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_states,elite_actions,elite_total_rewards=select_elites(s_log,a_log,G_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have 5 trajectories in the elite episodes\n",
    "\n",
    "e_0 : $s_0, a_0, r_0, s_1, a_1, ... $=> $\\hat G_1=-119$\n",
    "\n",
    "e_1 : $s_0, a_0, r_0, s_1, a_1, ... $=> $\\hat G_2=-206$\n",
    "\n",
    "e_2: $s_0, a_0, r_0, s_1, a_1, ... $=> $\\hat G_3=-360$\n",
    "\n",
    "...\n",
    "\n",
    "e_5: $s_0, a_0, r_0, s_1, a_1, ... $=> $\\hat G_5=-260$\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "How can we use it?\n",
    "\n",
    "\n",
    "We can update of our policy towards what the agent did in the elite episodes\n",
    "\n",
    "For instance, among all the trajectories, the agent visited state $s$ 230 times\n",
    "\n",
    "So we will have 230 action records that tell the agent what did it do in the state $s$\n",
    "\n",
    "Let's say\n",
    "\n",
    "- count(a=0)=200\n",
    "\n",
    "- count(a=1)=20\n",
    "\n",
    "- count(a=2)=10\n",
    "\n",
    "- count(a=other)=0\n",
    "\n",
    "now we can update the policy\n",
    "\n",
    "- $\\pi(a=0|s)=\\frac{200}{230}$\n",
    "\n",
    "- $\\pi(a=1|s)=\\frac{20}{230}$\n",
    "\n",
    "- $\\pi(a=2|s)=\\frac{10}{230}$\n",
    "\n",
    "- $\\pi(a=others|s)=\\frac{0}{230}$\n",
    "\n",
    "Then we can do the same for the policy of all other states in the elite episodes\n",
    "\n",
    "---\n",
    "\n",
    "One issue is updating the policy completely based on the trajectories might result in an immature convergence\n",
    "\n",
    "For instance\n",
    "\n",
    "a=5 in s might be the optimal action but since the policy is udpated to $\\pi(a=5|s)=0$, we will never be able to see this state-action record\n",
    "\n",
    "Therefore, we will do a soft update:\n",
    "\n",
    "$\\pi(a=0|s):=\\alpha \\frac{200}{230} + (1-\\alpha)\\pi(a=0|s)$, where $\\alpha\\in (0,1]$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote $\\hat S$ as a set of states that are in elite episodes\n",
    "\n",
    "Denote $\\hat G$ as a set of total rewards of elite episodes\n",
    "\n",
    "Denote $\\hat A$ as a set of actions of elite episodes\n",
    "\n",
    "Denote $\\alpha$ as the smooth parameter, $\\alpha \\in (0,1]$\n",
    "\n",
    "Denote $\\pi_k $ as the policy at iteration $k, k=0,1,2,...,\\hat k$\n",
    "\n",
    "$\\forall s\\in \\hat S, \\pi_0(a=0|s)=\\pi_0(a=1|s)=...=1/6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "0. k=0\n",
    "\n",
    "1. Play N episode under $\\pi_k$ and collect $\\hat S, \\hat A, and \\hat G$\n",
    "\n",
    "2. for $s \\in unique(\\hat S)$: \n",
    "\n",
    "    - Count the number of $a_k=0$, $a_k=1$,...,$a_k=5$ while $s_k=s$ within all elite episodes, denote them as $c_{a_0}$, $c_{a_1},...,c_{a_5}$\n",
    " \n",
    "    - Set:\n",
    "\n",
    "        $\\pi_{k+1}(a=0|s):=\\alpha \\frac{c_{a_0}}{\\sum_i c_{a_i}}+(1-\\alpha)\\pi_{k}(a=0|s)$\n",
    "\n",
    "        $\\pi_{k+1}(a=1|s):=\\alpha \\frac{c_{a_1}}{\\sum_i c_{a_i}}+(1-\\alpha)\\pi_{k}(a=1|s)$\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        $\\pi_{k+1}(a=5|s):=\\alpha \\frac{c_{a_5}}{\\sum_i c_{a_i}}+(1-\\alpha)\\pi_{k}(a=5|s)$\n",
    "\n",
    "3. stop if $k>\\hat k$, else, $k:=k+1$ go to step 1\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to update the policy based on the elite episodes\n",
    "def update_policy(elite_states,elite_actions,policy,num_states,num_actions,alpha=0.5):\n",
    "    \n",
    "    # make a copy of the current policy\n",
    "    new_policy = np.copy(policy)\n",
    "    \n",
    "    # list the unique states in the elite episodes\n",
    "    uniq_elite_states=list(set(elite_states))\n",
    "    \n",
    "    # going through each of the unique state\n",
    "    for s in uniq_elite_states:\n",
    "        \n",
    "        # list the actions did in the state s\n",
    "        actions_of_s=np.array(elite_actions)[np.array(elite_states)==s]\n",
    "        \n",
    "        # how many time did the agent visit the state s\n",
    "        num_records=len(actions_of_s)\n",
    "        \n",
    "        # check the c_a for each action\n",
    "        for a in range(num_actions):\n",
    "            \n",
    "            #count the number of actions a given the state s\n",
    "            num_a_in_s=len(actions_of_s[actions_of_s==a])\n",
    "            \n",
    "            #update the policy\n",
    "            prob_a_of_s=num_a_in_s/num_records\n",
    "            new_policy[s,a]=alpha*prob_a_of_s+(1-alpha)*policy[s,a]\n",
    "    \n",
    "    # return the updated policy\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states=env.observation_space.n\n",
    "num_actions=env.action_space.n\n",
    "policy = np.ones([num_states, num_actions]) / num_actions \n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_log,a_log,G_log =run(policy,iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expect_G,Std_G,threshold=np.mean(G_log),np.std(G_log),np.percentile(G_log,98)\n",
    "plt.hist(G_log,bins=30)\n",
    "plt.vlines(threshold,[0],[200],color=\"green\",label=\"98% percentile\")\n",
    "plt.vlines(Expect_G,[0],[200],color=\"black\",label=\"mean\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Expect reward:{:5.2f} | Std reward:{:5.2f} | Max reward:{} | Min reward:{}\".\\\n",
    "      format(Expect_G,Std_G,max(G_log),min(G_log)))\n",
    "print(\"98% percentile:{:5.2f}\".format(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_states,elite_actions,_=select_elites(s_log,a_log,G_log,percentile=98)\n",
    "len(elite_states),len(elite_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy=update_policy(elite_states,elite_actions,policy,num_states,num_actions,alpha=0.5)\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_log,a_log,G_log =run(policy,iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expect_G,Std_G,threshold=np.mean(G_log),np.std(G_log),np.percentile(G_log,98)\n",
    "plt.hist(G_log,bins=30)\n",
    "plt.vlines(threshold,[0],[200],color=\"green\",label=\"98% percentile\")\n",
    "plt.vlines(Expect_G,[0],[200],color=\"black\",label=\"mean\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Expect reward:{:5.2f} | Std reward:{:5.2f} | Max reward:{} | Min reward:{}\".\\\n",
    "      format(Expect_G,Std_G,max(G_log),min(G_log)))\n",
    "print(\"98% percentile:{:5.2f}\".format(threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(G_log,percentile):\n",
    "    plt.figure(figsize=(21,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    Expect_G,Std_G=np.mean(G_log),np.std(G_log)\n",
    "    meanG.append(Expect_G)\n",
    "    stdG.append(Std_G)\n",
    "    plt.hist(G_log,bins=30)\n",
    "    plt.xlabel(\"rewards\")\n",
    "    plt.ylabel(\"counts\")\n",
    "    plt.vlines(np.percentile(G_log,percentile),[0],[len(G_log)],color=\"green\",label=\"{}% percentile\".format(percentile))\n",
    "    plt.vlines(Expect_G,[0],[len(G_log)],color=\"black\",label=\"mean\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(range(len(meanG)),meanG,marker=\"o\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"mean reward\")\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(range(len(stdG)),stdG,marker=\"o\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"reward std\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print(\"Expect reward:{:5.2f} | Std reward:{:5.2f}\".format(Expect_G,Std_G))\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### CEM ###\n",
    "\n",
    "#init the random policy\n",
    "policy = np.ones([num_states, num_actions]) / num_actions \n",
    "\n",
    "#define a threshold\n",
    "percentile=50\n",
    "\n",
    "#define a learning rate\n",
    "alpha=0.5\n",
    "\n",
    "#init the records\n",
    "meanG,stdG=[],[]\n",
    "\n",
    "#define the number of iterations\n",
    "k_hat=50\n",
    "\n",
    "#update policy\n",
    "for k in range(k_hat):\n",
    "    \n",
    "    #sample trajectories under policy pi\n",
    "    s_log,a_log,G_log =run(policy,iterations=250)\n",
    "\n",
    "    #extract the elite episodes from sampled trajectories\n",
    "    elite_states,elite_actions,_=select_elites(s_log,a_log,G_log,percentile=percentile)   \n",
    "    \n",
    "    #update the policy\n",
    "    policy=update_policy(elite_states,elite_actions,policy,num_states,num_actions,alpha=alpha)\n",
    "    \n",
    "    #show the progress\n",
    "    show_progress(G_log,percentile)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what did the agent learn\n",
    "\n",
    "for _ in range(5):\n",
    "    G=0\n",
    "    s_t=env.reset()\n",
    "    k=0\n",
    "    while True:\n",
    "        a_t=np.random.choice(num_actions,p=policy[s_t])\n",
    "        s_t,r_t,done,info=env.step(a_t)\n",
    "        G+=r_t\n",
    "        k+=1\n",
    "        if not done:\n",
    "            env.render()\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            print(\"Game over! The total reward is {} ({})\".format(G,k))\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "<img src=\"files/recap_CEM.PNG\" height=\"100%\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "1. Discrete state space: we can discrete the continous state space (duck tape method, can be problematic); advanced the method would be learn a function to generate probability\n",
    "\n",
    "2. When the size of the state space goes too big, could be very slow: could learn an approximator\n",
    "\n",
    "3. Sample inefficiency: keep some elite experiences instead of regenerating every time\n",
    "\n",
    "4. Discrete action space:  could learn an approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the discrete state and action space case, $\\pi$ is a table\n",
    "\n",
    "| | a=0|a=1|a=2|...|a=m|\n",
    "|  ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| S=0| $\\frac{2}{10}$|$\\frac{0}{10}$|$\\frac{7}{10}$|...|$\\frac{1}{10}$|\n",
    "| S=1| $\\frac{15}{30}$|$\\frac{1}{30}$|$\\frac{5}{30}$|...|$\\frac{0}{30}$|\n",
    "| ...| ...| ...| ...| ...| ...|\n",
    "| S=n| $\\frac{2}{50}$|$\\frac{6}{50}$|$\\frac{17}{50}$|...|$\\frac{10}{50}$|\n",
    "\n",
    "However, when $s\\in R^d$, we can no longer create a table.\n",
    "\n",
    "So, we can parameterize the policy, say, $\\pi^\\theta(a|s)$, and for any $s$, we can extract the probability of taking action $a$.\n",
    "\n",
    "Example of parameterizing the policy:\n",
    "\n",
    "Say action space $A=\\{0,1\\}$, and state $s\\in R^d$\n",
    "\n",
    "$\\pi^\\theta(a=1|s)=\\frac{1}{1-e^{-\\theta^Ts}}$\n",
    "\n",
    "$\\pi^\\theta(a=0|s)=1-\\pi^\\theta(a=1|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main idea\n",
    "Parameterize the policy and learn the parameters\n",
    "\n",
    "<img src=\"files/deepCEM.PNG\" height=\"100%\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train $\\pi^\\theta(a|s)$ using $(s,a)$ pairs in elite episodes\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"CartPole-v1\")\n",
    "env._max_episode_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape,env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.high,env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions=env.action_space.n\n",
    "\n",
    "agent = MLPClassifier(hidden_layer_sizes=(64,32,8),activation='relu',warm_start=True,max_iter=1)\n",
    "\n",
    "agent.fit([env.reset()]*num_actions, list(range(num_actions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/NN_plot.PNG\" height=\"50%\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(agent,iterations):\n",
    "    S_LOG,A_LOG,G_LOG=[],[],[]\n",
    "    for iter in range(iterations):\n",
    "        G=0\n",
    "        st_log,at_log=[],[]\n",
    "        s_t=env.reset()\n",
    "        while True:\n",
    "            \n",
    "            ####\n",
    "            probs = agent.predict_proba([s_t])[0]\n",
    "            ####\n",
    "            \n",
    "            a_t=np.random.choice(num_actions,p=probs)\n",
    "            s_tp1,r_t,done,info=env.step(a_t)\n",
    "\n",
    "            st_log.append(s_t)\n",
    "            at_log.append(a_t)\n",
    "            G+=r_t\n",
    "\n",
    "            s_t=s_tp1\n",
    "\n",
    "            if done: \n",
    "                S_LOG.append(st_log)\n",
    "                A_LOG.append(at_log)\n",
    "                G_LOG.append(G)\n",
    "                break\n",
    "    return S_LOG,A_LOG,G_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile=90\n",
    "meanG=[]\n",
    "stdG=[]\n",
    "\n",
    "\n",
    "for k in range(100):\n",
    "    s_log,a_log,G_log =run(agent,iterations=100)\n",
    "    \n",
    "    elite_states,elite_actions,_=select_elites(s_log,a_log,G_log,percentile=percentile)   \n",
    "    \n",
    "    agent.fit(elite_states, elite_actions)\n",
    "    \n",
    "    show_progress(G_log,percentile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=0\n",
    "s_t=env.reset()\n",
    "k=0\n",
    "while True:\n",
    "    probs = agent.predict_proba([s_t])[0] \n",
    "    a_t=np.random.choice(num_actions,p=probs)\n",
    "    s_t,r_t,done,info=env.step(a_t)\n",
    "    G+=r_t\n",
    "    k+=1\n",
    "    if not done:\n",
    "        env.render()\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        print(\"Game over! The total reward is {} ({})\".format(G,k))\n",
    "        break\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "1. Sample inefficiency: keep some elite experiences instead of regenerating every time\n",
    "\n",
    "2. Discrete action space:  could learn an approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEM methods may not be the best method to use in many cases, but it is a good starting point to learn RL.\n",
    "\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. Dynamic programming is a good strating point if you want to learn more about RL\n",
    "\n",
    "2. Reinforcement learning by Sutton: https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
    "\n",
    "3. RL Course by David Silver (Deepmind): https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT\n",
    "\n",
    "4. RL course from UC Berkeley: https://www.youtube.com/playlist?list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">\n",
    "<img src=\"files/break.PNG\" height=\"100%\" width=\"100%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "326.533px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
